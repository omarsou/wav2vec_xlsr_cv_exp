{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"notebook_model.ipynb","provenance":[],"collapsed_sections":["2xpl09xvO7hm","zudrwLDGO9rY","5ipHCrkV2LHc","ami3ewqkUaD_","xmWQf7zq2fn8","GHIkAICG14lg","8V5iGi0g3Ac8","sgkxdSR-3caE","5mxfaO3p4_mp","pXTLSrSXI5oH","MC9nNI8B4FhH","6sYvc6m7gnrr","FYc-hol0RiDk","KnXGTrYehNUc"],"toc_visible":true,"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Ar6WxXxBzAuk"},"source":["This notebook is inspired from a very good [HuggingFace Tutorial](https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/Fine_Tune_XLSR_Wav2Vec2_on_Turkish_ASR_with_%F0%9F%A4%97_Transformers.ipynb#scrollTo=bTjNp2KUYAl8)"]},{"cell_type":"markdown","metadata":{"id":"2xpl09xvO7hm"},"source":["# pip install"]},{"cell_type":"code","metadata":{"id":"-3tuYZdqPADr"},"source":["!pip install phonemizer\n","!apt-get install espeak\n","!pip install git+https://github.com/huggingface/datasets.git\n","!pip install git+https://github.com/huggingface/transformers.git\n","!pip install torchaudio\n","!pip install librosa\n","!pip install jiwer"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zudrwLDGO9rY"},"source":["# notebook"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qQBWJA2jMNaD","executionInfo":{"status":"ok","timestamp":1616509568433,"user_tz":-60,"elapsed":873,"user":{"displayName":"Omar US","photoUrl":"","userId":"02556879631367095259"}},"outputId":"e28448bb-cc80-4148-be78-1e3fcf21a89c"},"source":["!nvidia-smi -L"],"execution_count":null,"outputs":[{"output_type":"stream","text":["GPU 0: Tesla P100-PCIE-16GB (UUID: GPU-a13a7848-cd36-d106-2117-85c6688bf2cb)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hlaKDvP4mpq6"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RK0f5qB2WylF"},"source":["# Import libraries\n","from datasets import load_dataset, load_metric, ClassLabel, load_from_disk\n","import datasets\n","datasets.set_caching_enabled(False)\n","\n","import torch\n","\n","from dataclasses import dataclass, field\n","\n","from typing import Any, Dict, List, Optional, Union\n","\n","from transformers import Wav2Vec2CTCTokenizer, Wav2Vec2FeatureExtractor, Wav2Vec2Processor, Wav2Vec2ForCTC\n","from transformers import AdamW, get_linear_schedule_with_warmup, get_polynomial_decay_schedule_with_warmup\n","\n","from torch.utils.data.dataloader import DataLoader\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","\n","import random\n","import math\n","import pandas as pd\n","import numpy as np\n","\n","from IPython.display import display, HTML\n","\n","import re\n","import json\n","import os\n","from tqdm.notebook import tqdm\n","\n","from utils import *\n","from trainer import Trainer"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5ipHCrkV2LHc"},"source":["# Vis"]},{"cell_type":"code","metadata":{"id":"gMRMKmC02Kib"},"source":["# Visualisation\n","def show_random_elements(dataset, num_examples=10):\n","    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n","    picks = []\n","    for _ in range(num_examples):\n","        pick = random.randint(0, len(dataset)-1)\n","        while pick in picks:\n","            pick = random.randint(0, len(dataset)-1)\n","        picks.append(pick)\n","    \n","    df = pd.DataFrame(dataset[picks])\n","    display(HTML(df.to_html()))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ami3ewqkUaD_"},"source":["# Load & Preprocess Dataset\n","IF YOU DON'T HAVE ALREADY THE DATASET PREPROCESSED CONTINUE, OTHERWISE SKIP THIS SECTION"]},{"cell_type":"markdown","metadata":{"id":"xmWQf7zq2fn8"},"source":["## Download/Load"]},{"cell_type":"markdown","metadata":{"id":"7ppiNca413CF"},"source":["First we are going to choose one language (you can look on https://huggingface.co/datasets/common_voice for other code's languages)"]},{"cell_type":"code","metadata":{"id":"B6jaK7bn2C42"},"source":["code_lang = \"cs\" # You can change if you want another language from the common voice dataset"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AQWsJshF2M6I"},"source":["For this experience, we chose Czech. \\\n","Let's download the dataset."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ILk9Yhi0OgvS","executionInfo":{"elapsed":2239,"status":"ok","timestamp":1616490079984,"user":{"displayName":"Omar US","photoUrl":"","userId":"02556879631367095259"},"user_tz":-60},"outputId":"977b6bf2-dc07-4d1d-bd6d-f2debc8ae3af"},"source":["common_voice = load_dataset(\"common_voice\", \"cs\", data_dir=\"./cv-corpus-6.1-2020-12-11\", split=\"train+validation\")\n","common_voice_test = load_dataset(\"common_voice\", \"cs\", data_dir=\"./cv-corpus-6.1-2020-12-11\", split=\"test\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[WARNING] Using custom data configuration cs-ad9f7b76efa9f3a0\n","[WARNING] Reusing dataset common_voice (/root/.cache/huggingface/datasets/common_voice/cs-ad9f7b76efa9f3a0/6.1.0/0041e06ab061b91d0a23234a2221e87970a19cf3a81b20901474cffffeb7869f)\n","[WARNING] Using custom data configuration cs-ad9f7b76efa9f3a0\n","[WARNING] Reusing dataset common_voice (/root/.cache/huggingface/datasets/common_voice/cs-ad9f7b76efa9f3a0/6.1.0/0041e06ab061b91d0a23234a2221e87970a19cf3a81b20901474cffffeb7869f)\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"9_LEaDDC2UHf"},"source":["If you are going to use only audio & transcription, you can remove the other columns."]},{"cell_type":"code","metadata":{"id":"SZJPv7tBPBTo"},"source":["common_voice = common_voice.remove_columns([\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"segment\", \"up_votes\"])\n","common_voice_test = common_voice_test.remove_columns([\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"segment\", \"up_votes\"])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GHIkAICG14lg"},"source":["## Preprocess\n","We are going to preprocess the dataset"]},{"cell_type":"code","metadata":{"id":"qnYPgxtU14GO"},"source":["show_random_elements(common_voice.remove_columns(['path']), num_examples=20)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yVun-Ioe209P"},"source":["Now, it depends on what we want to evaluate, if we want to evaluate the phonem transcription or the word transcription."]},{"cell_type":"markdown","metadata":{"id":"8V5iGi0g3Ac8"},"source":["### Word Transcription"]},{"cell_type":"markdown","metadata":{"id":"VFj3gHW33RGD"},"source":["We are going to preprocess the text and remove some special symbol `,.?!;` as we don't have any language model at the output"]},{"cell_type":"code","metadata":{"id":"BXn4PiA_3GLl"},"source":["chars_to_ignore_regex = '[\\,\\?\\.\\!\\-\\;\\:\\\"\\“\\%\\‘\\”\\�\\—\\…\\–\\«\\»]' # You can modify or add things here\n","common_voice = common_voice.map(remove_special_characters, remove_columns=[\"sentence\"])\n","common_voice_test = common_voice_test.map(remove_special_characters, remove_columns=[\"sentence\"])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aJw66R_w3aj0"},"source":["show_random_elements(common_voice.remove_columns(['path']), num_examples=20)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sgkxdSR-3caE"},"source":["### Phonem Transcription"]},{"cell_type":"markdown","metadata":{"id":"5XI5VWmB3fe1"},"source":["For phonem transcription we need first to convert the text to phonemes."]},{"cell_type":"code","metadata":{"id":"tUFCciNKR8YG"},"source":["common_voice = common_voice.map(text2phoneme, num_proc=4)\n","common_voice_test = common_voice_test.map(text2phoneme, num_proc=4)\n","common_voice = common_voice.rename_column(\"sentence\", \"text\")\n","common_voice_test = common_voice_test.rename_column(\"sentence\", \"text\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UrjmzIbZSdO-"},"source":["show_random_elements(common_voice.remove_columns(['path']), num_examples=20)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5mxfaO3p4_mp"},"source":["## Building Vocabulary"]},{"cell_type":"markdown","metadata":{"id":"d7GXUJ3S5BUT"},"source":["As we are going to use a CTC (as top layer), we are going to classify speech chunks into letters, so now we will extract all distinct letters and build our vocabulary from that."]},{"cell_type":"code","metadata":{"id":"2kgxTh1U5VNL"},"source":["vocab_train = common_voice.map(extract_all_chars, batched=True, batch_size=-1, keep_in_memory=True, remove_columns=common_voice.column_names)\n","vocab_test = common_voice_test.map(extract_all_chars, batched=True, batch_size=-1, keep_in_memory=True, remove_columns=common_voice_test.column_names)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5YNn85Yi5i1r"},"source":["Now we will create the union of all distinct letters from both dataset. We will do the same thing as when we are dealing with translation / generation task."]},{"cell_type":"code","metadata":{"id":"Z2CwjQQv5uho"},"source":["vocab_list = list(set(vocab_train[\"vocab\"][0]) | set(vocab_test[\"vocab\"][0]))\n","vocab_dict = {v: k for k, v in enumerate(vocab_list)}\n","vocab_dict"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0FdMWUD4HOvy"},"source":["# Adding the blank token, the unknown token and the padding token\n","vocab_dict[\"|\"] = vocab_dict[\" \"]\n","del vocab_dict[\" \"]\n","vocab_dict[\"[UNK]\"] = len(vocab_dict)\n","vocab_dict[\"[PAD]\"] = len(vocab_dict)\n","print(f\"Our final layer will have as output dimension {len(vocab_dict)}\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8Z0wt0YQHnDC"},"source":["# Now let's save our dictionary\n","parent_dir = \"...\" # Here you have to put where you want to save the vocabulary\n","with open(os.path.join(parent_dir, 'czeck_phonem_vocab.json'), 'w') as vocab_file:\n","    json.dump(vocab_dict, vocab_file)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pXTLSrSXI5oH"},"source":["## Audio Preprocessing"]},{"cell_type":"markdown","metadata":{"id":"xS74MNgZ4Bpy"},"source":["Now we are going to open and store the audio file (represented as a numpy array)"]},{"cell_type":"code","metadata":{"id":"PDvLUjQ6JoMw"},"source":["common_voice = common_voice.map(speech_file_to_array_fn, remove_columns=common_voice.column_names)\n","common_voice_test = common_voice_test.map(speech_file_to_array_fn, remove_columns=common_voice_test.column_names)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MC9nNI8B4FhH"},"source":["### Resample\n","**If you dataset is already sampled to 16kHZ, skip this step** \\"]},{"cell_type":"markdown","metadata":{"id":"zb_XdY0o5X3a"},"source":["Wav2Vec2 (XLSR or English Only) was pretrained on the audio data of Babel, Multilingual LibriSpeech (MLS), and Common Voice. Most of those datasets were sampled at 16kHz, so that Common Voice, sampled at 48kHz, has to be downsampled to 16kHz for training. Therefore, we will have to downsample our fine-tuning data to 16kHz in the following."]},{"cell_type":"code","metadata":{"id":"QeJR0xmGI_uo"},"source":["# First we have to downsampled the original sample from 48 kHZ to 16kHZ\n","common_voice = common_voice.map(resample, num_proc=4)\n","common_voice_test = common_voice_test.map(resample, num_proc=4)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6sYvc6m7gnrr"},"source":["# Train Dev Test\n","Now we are going to split our data intro three subsets. Fortunately, the common voice already provide us with these subset. \\\n","Nevertheless if you want to do your own split, you can follow these steps (note that I will not modify the test set as it is already given by CV, so it's better to keep the same testing set in order to have a fair and good comparison)"]},{"cell_type":"code","metadata":{"id":"KhqVRMLQFPTl"},"source":["# Split into train/dev\n","np.random.seed(42)\n","data = common_voice.train_test_split(test_size=0.2, seed=42)\n","common_voice_train, common_voice_validation = data['train'], data['test']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ejYg7v1853oi"},"source":["Now if you want to make experimentation (as I did) and see how these pretrained models performs with few labeled data, you can split the train into different subsets (10mn, 1h, 10h for instance)"]},{"cell_type":"code","metadata":{"id":"fcExOxBNf1PB"},"source":["# Now let's shuffle data\n","common_voice_train = common_voice_train.shuffle(seed=42)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gAXOZJNKedmh"},"source":["total_len_seconds = 0\n","indices_10mn = []\n","indices_1h = []\n","indices_10h = []\n","for i in tqdm(range(len(common_voice_train))):\n","  speech_array, sampling_rate = common_voice_train[i][\"speech\"], common_voice_train[i][\"sampling_rate\"]\n","  duration_audio = len(speech_array) * (1/sampling_rate)\n","  if total_len_seconds <= 600: # 600 => 10 minutes\n","    indices_10mn.append(i)\n","  if total_len_seconds <= 3600: # 3600 => 60 minutes => 1 heure\n","    indices_1h.append(i)\n","  if total_len_seconds <= 36000: # 36000 => 600 minutes => 10 heures\n","    indices_10h.append(i)\n","  if total_len_seconds > 36000:\n","    break\n","  total_len_seconds += duration_audio  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"p8HbeUcrk9Ih"},"source":["common_voice_train_10mn = common_voice_train.select(indices_10mn)\n","common_voice_train_1h = common_voice_train.select(indices_1h)\n","common_voice_train_10h = common_voice_train.select(indices_10h)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"r_s1mKyrRkFd"},"source":["common_voice_train_10mn = common_voice_train_10mn.map(prepare_dataset, remove_columns=common_voice_train_10mn.column_names, batch_size=8, num_proc=4, batched=True)\n","common_voice_train_1h = common_voice_train_1h.map(prepare_dataset, remove_columns=common_voice_train_1h.column_names, batch_size=8, num_proc=4, batched=True)\n","common_voice_train_10h = common_voice_train_10h.map(prepare_dataset, remove_columns=common_voice_train_10h.column_names, batch_size=8, num_proc=4, batched=True)\n","common_voice_train = common_voice_train.map(prepare_dataset, remove_columns=common_voice_train.column_names, batch_size=8, num_proc=4, batched=True)\n","common_voice_validation = common_voice_validation.map(prepare_dataset, remove_columns=common_voice_validation.column_names, batch_size=8, num_proc=4, batched=True)\n","common_voice_test = common_voice_test.map(prepare_dataset, remove_columns=common_voice_test.column_names, batch_size=8, num_proc=4, batched=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I_XBvfGl62YN"},"source":["We can to disk the data ... but **you will need extra memory as the files are huge**"]},{"cell_type":"code","metadata":{"id":"EThqyYL-6sZu"},"source":["common_voice_train_10mn.save_to_disk(os.path.join(parent_dir, 'train_czeck_phonem_10mn.files')\n","common_voice_train_1h.save_to_disk(os.path.join(parent_dir, 'train_czeck_phonem_1h.files'))\n","common_voice_train_10h.save_to_disk(os.path.join(parent_dir, 'train_czeck_phonem_10h.files'))\n","common_voice_train.save_to_disk(os.path.join(parent_dir, 'train_czeck_phonem.files'))\n","common_voice_validation.save_to_disk(os.path.join(parent_dir, 'validation_czeck_phonem.files'))\n","common_voice_test.save_to_disk(os.path.join(parent_dir, 'test_czeck_phonem.files'))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FYc-hol0RiDk"},"source":["# Training"]},{"cell_type":"code","metadata":{"id":"GLl65vF78B0i"},"source":["parent_dir = '...' # Your path"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bLZPYC1O7Ooo"},"source":["common_voice_train_10mn = load_from_disk(os.path.join(parent_dir, 'train_czeck_phonem_10mn.files')\n","common_voice_train_1h = load_from_disk(os.path.join(parent_dir, 'train_czeck_phonem_1h.files'))\n","common_voice_train_10h = load_from_disk(os.path.join(parent_dir, 'train_czeck_phonem_10h.files'))\n","common_voice_train = load_from_disk(os.path.join(parent_dir, 'train_czeck_phonem.files'))\n","common_voice_validation = load_from_disk(os.path.join(parent_dir, 'validation_czeck_phonem.files'))\n","common_voice_test = load_from_disk(os.path.join(parent_dir, 'test_czeck_phonem.files'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YcU0OqWiCITf"},"source":["# Loading tokenizer\n","tokenizer = Wav2Vec2CTCTokenizer(os.path.join(parent_dir, 'czeck_phonem_vocab.json'), unk_token=\"[UNK]\", pad_token=\"[PAD]\", word_delimiter_token=\"|\")\n","# Load Feature Extractor\n","feature_extractor = Wav2Vec2FeatureExtractor(feature_size=1, sampling_rate=16000, padding_value=0.0, do_normalize=True, return_attention_mask=True)\n","# Wrap the feature_extractor and the tokenizer into one class (thanks so much HuggingFace)\n","processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)\n","# Prepare our data collator\n","data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ADs7vPCeV84t"},"source":["The first component of XLSR-Wav2Vec2 consists of a stack of CNN layers that are used to extract acoustically meaningful - but contextually independent - features from the raw speech signal. This part of the model has already been sufficiently trained during pretraining and as stated in the [paper](https://arxiv.org/pdf/2006.13979.pdf) does not need to be fine-tuned anymore. \n","Thus, we can set the `requires_grad` to `False` for all parameters of the *feature extraction* part."]},{"cell_type":"code","metadata":{"id":"98jur6H-exB1"},"source":["# Load model\n","model = Wav2Vec2ForCTC.from_pretrained(\n","    \"facebook/wav2vec2-large-xlsr-53\", \n","    # \"facebook/wav2vec2-base-960h\",\n","    attention_dropout=0.1,\n","    hidden_dropout=0.1,\n","    feat_proj_dropout=0.0,\n","    mask_time_prob=0.05,\n","    layerdrop=0.1,\n","    gradient_checkpointing=True, \n","    ctc_loss_reduction=\"mean\", \n","    pad_token_id=processor.tokenizer.pad_token_id,\n","    vocab_size=len(processor.tokenizer)\n",")\n","\n","# Freeze the feature extractor\n","model.freeze_feature_extractor()\n","\n","# Set to GPU\n","model.cuda()\n","\n","# Get sampler\n","model_input_name = processor.feature_extractor.model_input_names[0]\n","sampler_train = trainer_pt_utils.LengthGroupedSampler(common_voice_train_10mn, batch_size=12, model_input_name=model_input_name)\n","sampler_val = trainer_pt_utils.LengthGroupedSampler(common_voice_validation, batch_size=12, model_input_name=model_input_name)\n","\n","# Get Loader\n","train_loader = DataLoader(common_voice_train_10mn, batch_size=12, sampler=sampler_train, collate_fn=data_collator, num_workers=4)\n","valid_loader = DataLoader(common_voice_validation, batch_size=12, sampler=sampler_val, collate_fn=data_collator, num_workers=4)\n","\n","#\n","learning_rate = 4e-4\n","n_epochs = 350\n","\n","num_update_steps_per_epoch = len(train_loader)\n","max_steps = math.ceil(n_epochs * num_update_steps_per_epoch)\n","validation_freq = int(1*num_update_steps_per_epoch)\n","print_freq = int(1*num_update_steps_per_epoch)\n","scheduler_on_plateau_freq = int(num_update_steps_per_epoch)\n","\n","# Optimizer\n","decay_parameters = trainer_pt_utils.get_parameter_names(model, [torch.nn.LayerNorm])\n","decay_parameters = [name for name in decay_parameters if \"bias\" not in name]\n","optimizer_grouped_parameters = [\n","    {\n","        \"params\": [p for n, p in model.named_parameters() if n in decay_parameters],\n","        \"weight_decay\": 0.0,\n","    },\n","    {\n","        \"params\": [p for n, p in model.named_parameters() if n not in decay_parameters],\n","        \"weight_decay\": 0.0,\n","    },\n","]\n","optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n","\n","# Scheduler\n","num_warmup_steps = int(50 * num_update_steps_per_epoch) # Neccessary Number of steps to go from 0.0 to lr \n","#warmup_scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps, max_steps)\n","warmup_scheduler = get_polynomial_decay_schedule_with_warmup(optimizer, num_warmup_steps, max_steps, lr_end=1e-7)\n","reduce_lr_plateau = None\n","## reduce_lr_plateau = ReduceLROnPlateau(optimizer, factor=0.6, patience=7) ## To define when warmup scheduler is finished\n","\n","model.zero_grad() \n","current_total_steps = 0\n","current_best_wer = 2.0\n","\n","trainer = Trainer(model, processor, optimizer, warmup_scheduler, validation_freq, print_freq, num_warmup_steps, False, type_score='PER')\n","trainer.train(train_loader, valid_loader, n_epochs, path=\"/content/examples/model.pt\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KnXGTrYehNUc"},"source":["# Result on Test Set"]},{"cell_type":"code","metadata":{"id":"h5waSufMl5It"},"source":["sampler_test = trainer_pt_utils.LengthGroupedSampler(common_voice_test, batch_size=12, model_input_name=model_input_name)\n","test_loader = DataLoader(common_voice_test, batch_size=12, sampler=sampler_test, collate_fn=data_collator, num_workers=4)\n","print(f\"The final PER score on the test set is {trainer.compute_score(test_loader, \"/content/examples/model.pt\")}\")"],"execution_count":null,"outputs":[]}]}